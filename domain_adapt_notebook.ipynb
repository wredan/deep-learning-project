{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "094ab084",
   "metadata": {},
   "source": [
    "# Domain adaptation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8c487c",
   "metadata": {},
   "source": [
    "### Initial settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b79da6",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36870725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement zipfile (from versions: none)\n",
      "ERROR: No matching distribution found for zipfile\n",
      "WARNING: You are using pip version 21.1.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Alessandro\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user -qr ./requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e92c0f3",
   "metadata": {},
   "source": [
    "#### Import and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe092eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "# your favorite machine learning tracking tool\n",
    "# from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import wget \n",
    "import numpy as np\n",
    "# import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5134e9fb",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d669ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: spostare questa implementazione in un file a parte\n",
    "import os\n",
    "import zipfile\n",
    "from scripts.extract_patches import *\n",
    "import shutil\n",
    "\n",
    "class CulturalSiteDatasetsLoader():\n",
    "\n",
    "    def __init__(self, download_path):    \n",
    "        self._class_path_datasets = './CLASS-EGO-CH-OBJ-ADAPT/' # TODO: non è detto che si chiami così, configurarlo in modo che sia così\n",
    "        if not self._classification_datasets_exists():\n",
    "            self._main_path_datasets = './EGO-CH-OBJ-ADAPT/'\n",
    "            if not self._main_datasets_exists():\n",
    "                self._get_main_datasets(download_path) \n",
    "            self._extract_patches(self._main_path_datasets, self._class_path_datasets)    \n",
    "            shutil.rmtree(self._main_path_datasets) # remove Main Dataset #TODO: add default value to do this\n",
    "\n",
    "    def _get_main_datasets(self, download_path): # download and extract dataset\n",
    "        save_path = \"./\"\n",
    "        wget.download(download_path, save_path)\n",
    "        with zipfile.ZipFile(self._main_path_datasets, 'r') as zip_ref:\n",
    "            zip_ref.extractall(save_path)\n",
    "        os.remove('./EGO-CH-OBJ-ADAPT.zip')\n",
    "\n",
    "    def _extract_patches(self, main_path, save_path):\n",
    "        ExtractPatches(main_path, save_path)\n",
    "\n",
    "    def _main_datasets_exists(self):\n",
    "        return os.path.isdir(self._main_path_datasets)\n",
    "\n",
    "    def _classification_datasets_exists(self):\n",
    "        return os.path.isdir(self._class_path_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd46f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "import json\n",
    "from PIL import Image\n",
    "from torchvision.datasets.utils import check_integrity, download_and_extract_archive\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "\n",
    "class CulturalSiteDataset(VisionDataset):\n",
    "    def __init__(self, dataset_folder : str, transform: Optional[Callable] = None) -> None:\n",
    "        super().__init__(root = dataset_folder, transform = transform, target_transform=None)\n",
    "        self.images_data = []\n",
    "        self.image_classes = []\n",
    "        # self._load_images(dataset_folder)\n",
    "        self._load_image_classes(\"./utils/image_classes.json\") # todo: remove hardcode (add config file)\n",
    "        self._load_class_ids(\"./utils/image_classes.json\")     # todo: remove hardcode o salvare \n",
    "        # todo: spostare in gpu se disponile\n",
    "\n",
    "    def _load_images(self, path):\n",
    "        for filename in os.listdir(path):\n",
    "            im = Image.open(os.path.join(path, filename))\n",
    "            self.images_data.append(np.asarray(im))\n",
    "\n",
    "    def _load_image_classes(self, path):\n",
    "        file = open(path)\n",
    "        content = json.load(file)\n",
    "        for key,el in content[\"categories\"]:\n",
    "            self.image_classes.append(el)\n",
    "\n",
    "    def _load_class_ids(self, path):  # TODO: ottimizzare se possibile\n",
    "        file = open(path)\n",
    "        content = json.load(file)\n",
    "        for i in range(len(self.image_classes)):\n",
    "            for el in content[\"categories\"]:\n",
    "                if el[\"name\"] == self.image_classes[i]:\n",
    "                    self.image_classes[i] = el[\"id\"]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img, image_class = self.images_data[index], self.image_classes[index]\n",
    "        img = Image.fromarray(img)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform()\n",
    "        return img, image_class\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "181858e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CulturalSiteDataset\n",
       "    Number of datapoints: 0\n",
       "    Root location: ./CLASS-EGO-CH-OBJ-ADAPT/syntehtic/test/data/"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEST Dataset Cell\n",
    "\n",
    "CulturalSiteDataset(\"./CLASS-EGO-CH-OBJ-ADAPT/syntehtic/test/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28bb905c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'check_integrity' from 'torchvision.utils' (c:\\Users\\Alessandro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\Desktop\\university\\progetti\\DeepLearning\\deep-learning-project\\domain_adapt_notebook.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/university/progetti/DeepLearning/deep-learning-project/domain_adapt_notebook.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/university/progetti/DeepLearning/deep-learning-project/domain_adapt_notebook.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Desktop/university/progetti/DeepLearning/deep-learning-project/domain_adapt_notebook.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m check_integrity, download_and_extract_archive\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/university/progetti/DeepLearning/deep-learning-project/domain_adapt_notebook.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# from .vision import VisionDataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/university/progetti/DeepLearning/deep-learning-project/domain_adapt_notebook.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCIFAR10\u001b[39;00m(VisionDataset):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'check_integrity' from 'torchvision.utils' (c:\\Users\\Alessandro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\utils.py)"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "from typing import Any, Callable, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.datasets.utils import check_integrity, download_and_extract_archive\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "\n",
    "class CIFAR10(VisionDataset):\n",
    "    \"\"\"`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where directory\n",
    "            ``cifar-10-batches-py`` exists or will be saved to if download is set to True.\n",
    "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
    "            creates from test set.\n",
    "        transform (callable, optional): A function/transform that takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    base_folder = \"cifar-10-batches-py\"\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    filename = \"cifar-10-python.tar.gz\"\n",
    "    tgz_md5 = \"c58f30108f718f92721af3b95e74349a\"\n",
    "    train_list = [\n",
    "        [\"data_batch_1\", \"c99cafc152244af753f735de768cd75f\"],\n",
    "        [\"data_batch_2\", \"d4bba439e000b95fd0a9bffe97cbabec\"],\n",
    "        [\"data_batch_3\", \"54ebc095f3ab1f0389bbae665268c751\"],\n",
    "        [\"data_batch_4\", \"634d18415352ddfa80567beed471001a\"],\n",
    "        [\"data_batch_5\", \"482c414d41f54cd18b22e5b47cb7c3cb\"],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        [\"test_batch\", \"40351d587109b95175f43aff81a1287e\"],\n",
    "    ]\n",
    "    meta = {\n",
    "        \"filename\": \"batches.meta\",\n",
    "        \"key\": \"label_names\",\n",
    "        \"md5\": \"5ff9c542aee3614f3951f8cda6e48888\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        train: bool = True,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "        download: bool = False,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__(root, transform=transform, target_transform=target_transform)\n",
    "\n",
    "        self.train = train  # training set or test set\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError(\"Dataset not found or corrupted. You can use download=True to download it\")\n",
    "\n",
    "        if self.train:\n",
    "            downloaded_list = self.train_list\n",
    "        else:\n",
    "            downloaded_list = self.test_list\n",
    "\n",
    "        self.data: Any = []\n",
    "        self.targets = []\n",
    "\n",
    "        # now load the picked numpy arrays\n",
    "        for file_name, checksum in downloaded_list:\n",
    "            file_path = os.path.join(self.root, self.base_folder, file_name)\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                entry = pickle.load(f, encoding=\"latin1\")\n",
    "                self.data.append(entry[\"data\"])\n",
    "                if \"labels\" in entry:\n",
    "                    self.targets.extend(entry[\"labels\"])\n",
    "                else:\n",
    "                    self.targets.extend(entry[\"fine_labels\"])\n",
    "\n",
    "        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32)\n",
    "        self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "\n",
    "        self._load_meta()\n",
    "\n",
    "    def _load_meta(self) -> None:\n",
    "        path = os.path.join(self.root, self.base_folder, self.meta[\"filename\"])\n",
    "        if not check_integrity(path, self.meta[\"md5\"]):\n",
    "            raise RuntimeError(\"Dataset metadata file not found or corrupted. You can use download=True to download it\")\n",
    "        with open(path, \"rb\") as infile:\n",
    "            data = pickle.load(infile, encoding=\"latin1\")\n",
    "            self.classes = data[self.meta[\"key\"]]\n",
    "        self.class_to_idx = {_class: i for i, _class in enumerate(self.classes)}\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def _check_integrity(self) -> bool:\n",
    "        root = self.root\n",
    "        for fentry in self.train_list + self.test_list:\n",
    "            filename, md5 = fentry[0], fentry[1]\n",
    "            fpath = os.path.join(root, self.base_folder, filename)\n",
    "            if not check_integrity(fpath, md5):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def download(self) -> None:\n",
    "        if self._check_integrity():\n",
    "            print(\"Files already downloaded and verified\")\n",
    "            return\n",
    "        download_and_extract_archive(self.url, self.root, filename=self.filename, md5=self.tgz_md5)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        split = \"Train\" if self.train is True else \"Test\"\n",
    "        return f\"Split: {split}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1674bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CulturalSiteDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "            # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        CulturalSiteDatasetsLoader(\"https://iplab.dmi.unict.it/EGO-CH-OBJ-ADAPT/EGO-CH-OBJ-ADAPT.zip\") # todo: rimuovere hardcode\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        # TODO: qui istanziare CulturalSiteDataset, creare i dataset da passare ai dataloader sotto\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == 'fit' or stage is None:\n",
    "            cifar_full = CIFAR10(self.data_dir, train=True, transform=self.transform)\n",
    "            self.cifar_train, self.cifar_val = random_split(cifar_full, [45000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.cifar_test = CIFAR10(self.data_dir, train=False, transform=self.transform)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.cifar_train, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.cifar_val, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.cifar_test, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badb5eff",
   "metadata": {},
   "source": [
    "### Dataset Pre Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62601f8d",
   "metadata": {},
   "source": [
    "*todo*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bcf0bc",
   "metadata": {},
   "source": [
    "### Normalize input features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80282f07",
   "metadata": {},
   "source": [
    "*todo*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7f500a",
   "metadata": {},
   "source": [
    "## Domain adaptation study cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dfb075",
   "metadata": {},
   "source": [
    "### 1. Baseline approaches without adaption\n",
    "Il primo caso di studio è quello di allenare il classificatore sul sintetico e poi testarlo nudo e crudo sul reale, valutare le performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc1d39",
   "metadata": {},
   "source": [
    "#### Init Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aaefd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePredictionLogger(pl.callbacks.Callback):\n",
    "    def __init__(self, val_samples, num_samples=32):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.val_imgs, self.val_labels = val_samples\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        # Bring the tensors to CPU\n",
    "        val_imgs = self.val_imgs.to(device=pl_module.device)\n",
    "        val_labels = self.val_labels.to(device=pl_module.device)\n",
    "        # Get model prediction\n",
    "        logits = pl_module(val_imgs)\n",
    "        preds = torch.argmax(logits, -1)\n",
    "        # Log the images as wandb Image\n",
    "        ''' trainer.logger.experiment.log({\n",
    "            \"examples\":[wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\") \n",
    "                           for x, pred, y in zip(val_imgs[:self.num_samples], \n",
    "                                                 preds[:self.num_samples], \n",
    "                                                 val_labels[:self.num_samples])]\n",
    "            }) '''\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd53f0a",
   "metadata": {},
   "source": [
    "### Classification Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d19868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, input_shape, num_classes, learning_rate=2e-4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # log hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3, 1)\n",
    "\n",
    "        self.pool1 = torch.nn.MaxPool2d(2)\n",
    "        self.pool2 = torch.nn.MaxPool2d(2)\n",
    "        \n",
    "        n_sizes = self._get_conv_output(input_shape)\n",
    "\n",
    "        self.fc1 = nn.Linear(n_sizes, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "        self.accuracy = Accuracy()\n",
    "\n",
    "    # returns the size of the output tensor going into Linear layer from the conv block.\n",
    "    def _get_conv_output(self, shape):\n",
    "        batch_size = 1\n",
    "        input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
    "\n",
    "        output_feat = self._forward_features(input) \n",
    "        n_size = output_feat.data.view(batch_size, -1).size(1)\n",
    "        return n_size\n",
    "        \n",
    "    # returns the feature tensor from the conv block\n",
    "    def _forward_features(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool2(F.relu(self.conv4(x)))\n",
    "        return x\n",
    "    \n",
    "    # will be used during inference\n",
    "    def forward(self, x):\n",
    "       x = self._forward_features(x)\n",
    "       x = x.view(x.size(0), -1)\n",
    "       x = F.relu(self.fc1(x))\n",
    "       x = F.relu(self.fc2(x))\n",
    "       x = F.log_softmax(self.fc3(x), dim=1)\n",
    "       \n",
    "       return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        \n",
    "        # training metrics\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.accuracy(preds, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # validation metrics\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.accuracy(preds, y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        \n",
    "        # validation metrics\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.accuracy(preds, y)\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddec19c",
   "metadata": {},
   "source": [
    "### Preparing DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529545bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = CIFAR10DataModule(batch_size=32) # todo: istanziare CulturalSiteDataset, preparare i dati\n",
    "# To access the x_dataloader we need to call prepare_data and setup.\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "\n",
    "# Samples required by the custom ImagePredictionLogger callback to log image predictions.\n",
    "val_samples = next(iter(dm.val_dataloader()))\n",
    "val_imgs, val_labels = val_samples[0], val_samples[1]\n",
    "val_imgs.shape, val_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9faef35",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8dd919",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitModel((3, 32, 32), dm.num_classes)\n",
    "\n",
    "# todo: inizializzare il logger, utilizzeremo tensorboard come anno scorso\n",
    "# Initialize wandb logger\n",
    "# wandb_logger = WandbLogger(project='wandb-lightning', job_type='train')\n",
    "\n",
    "# Initialize Callbacks\n",
    "early_stop_callback = pl.callbacks.EarlyStopping(monitor=\"val_loss\")\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint()\n",
    "\n",
    "# Initialize a trainer\n",
    "# todo: vedere se possibile riconoscere automaticamente il device\n",
    "trainer = pl.Trainer(max_epochs=10,\n",
    "                     accelerator=\"mps\", #'mps' to use apple silicon graphics unit, 'gpu' for nvidia or amd \n",
    "                     devices=1,\n",
    "                     # logger=wandb_logger,\n",
    "                     callbacks=[early_stop_callback,\n",
    "                                ImagePredictionLogger(val_samples),\n",
    "                                checkpoint_callback],\n",
    "                     )\n",
    "\n",
    "# Train the model ⚡🚅⚡\n",
    "trainer.fit(model, dm)\n",
    "\n",
    "# Evaluate the model on the held-out test set ⚡⚡\n",
    "trainer.test(dataloaders=dm.test_dataloader())\n",
    "\n",
    "# Close wandb run\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca146b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1b9fe44b3991069bed7f12c115f8ef5897ca9ea72b00cc34904974e146407600"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
